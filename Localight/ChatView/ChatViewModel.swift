//
//  ChatViewModel.swift
//  Localight
//
//  Created by Timo Köthe on 07.07.25.
//

import Foundation
import FoundationModels

/// `ChatViewModel` is an observable class that manages the state and logic for a chat interface utilizing a language model session.
/// 
/// Responsibilities include:
/// - Managing user input and chat messages.
/// - Interfacing with `LanguageModelSession` to send prompts and handle responses.
/// - Tracking the status of responses and updating the UI accordingly.
/// 
/// Properties:
/// - `session`: Handles communication with the language model, initialized with specific instructions.
/// - `options`: Defines some runtime parameters of the model.
/// - `instructions`: Defines how the model should respond and behave.
/// - `temperature`: Defines the creativity of the model.
/// - `samplingMode`: Defines how the model chooses the next token in a response.
/// - `inputText`: The current text entered by the user.
/// - `prompt`: Stores the current prompt sent to the language model.
/// - `isResponding`: Indicates whether a response is currently being generated by the model.
/// - `isStreaming`: Indicates whether a response should be streamed to the UI or not.
/// - `messages`: The list of chat messages exchanged between the user and the model.
/// - `streamingResponse`: The current chunk of text for the UI of a streamed response by the model.
///
/// Methods:
/// - `getResponse()`: Asynchronously sends the current prompt to the language model and appends both user and model messages to the chat, handling errors gracefully.
/// - `streamResponse()`: Asynchronously sends the current prompt to the language model and streams the models response to the chat while being generated, handling errors gracefully.
/// - `resetSession()`: Resets the current language model session by setting all variables to their initial values.
@Observable class ChatViewModel {
    private var session: LanguageModelSession
    private var options: GenerationOptions
    
    var instructions: String    // The instructions to the model of how it should respond and behave.
    var temperature: Double     // The temperature to increase creativity.
    var samplingMode: GenerationOptions.SamplingMode
    var inputText: String       // The text currently entered by the user.
    var prompt: String          // The finalized user input sent to the model.
    var isResponding: Bool      // Indicates whether the model is generating a response.
    var isStreaming: Bool       // Indicates whether a response should be streamed to the UI or not.
    var messages: [Message]     // A collection of all messages displayed in the chat view.
    var streamingResponse: String
    
    /// Initializes all variables with their values.
    init() {
        self.session = LanguageModelSession(instructions: "Act as the best buddie. Keep your answer short.")
        self.options = GenerationOptions(sampling: .greedy, temperature: 2.0)
        self.instructions = "Act as the best buddie. Keep your answer short."
        self.temperature = 2.0
        self.samplingMode = .greedy
        self.inputText = ""
        self.prompt = ""
        self.isResponding = false
        self.isStreaming = false
        self.messages = []
        self.streamingResponse = ""
    }
    
    /// Generates a response from the model based on the user’s current input.
    ///
    /// This function handles the full request–response cycle:
    /// - Marks the view as busy (`isResponding = true`)
    /// - Appends the user’s input as a message to be shown on the ChatView
    /// - Sends the finalized prompt to the model
    /// - Awaits the model’s response, or an error message if the request fails
    /// - Resets the state to indicate the response cycle has finished
    func getResponse() async {
        isResponding = true
        messages.append(Message(text: inputText, sender: .user))
        prompt = inputText
        inputText = ""
        do {
            let response = try await session.respond(to: prompt).content
            let message = Message(text: response, sender: .model)
            messages.append(message)
        } catch {
            let message = Message(text: error.localizedDescription, sender: .model)
            messages.append(message)
        }
        isResponding = false
    }
    
    /// Streams a response from the model based on the user’s current input.
    ///
    /// This function handles the full request–response cycle:
    /// - Marks the view as busy (`isResponding = true`)
    /// - Appends the user’s input as a message to be shown on the ChatView
    /// - Sends the finalized prompt to the model
    /// - Streams the model’s response, or appends an error message if the request fails
    /// - Resets the state to indicate the response cycle has finished
    func streamResponse() async {
        isResponding = true
        isResponding = true
        messages.append(Message(text: inputText, sender: .user))
        prompt = inputText
        inputText = ""
        let stream = session.streamResponse(to: prompt)
        do {
            for try await chunk in stream {
                self.streamingResponse = chunk.content
            }
            
            let response = try await stream.collect().content
            let message = Message(text: response, sender: .model)
            messages.append(message)
        } catch {
            let message = Message(text: error.localizedDescription, sender: .model)
            messages.append(message)
        }
        streamingResponse = ""
        isResponding = false
        print(session.transcript)
    }
    
    /// Resets the current session to initial values.
    ///
    /// This function resets the current session by reseting all variables to their initial values.
    func resetSession() {
        self.session = LanguageModelSession(instructions: self.instructions)
        self.options = GenerationOptions(sampling: .greedy, temperature: self.temperature)
        self.inputText = ""
        self.prompt = ""
        self.isResponding = false
        self.isStreaming = false
        self.messages = []
        self.streamingResponse = ""
    }
}
